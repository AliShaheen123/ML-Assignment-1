# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cuer200Obli1mUyyTqQhcTf_m6TwRo4N

### Basic Steps
"""

# Importing the used modules. 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""#### Data loading"""

# Reading and  the datasets as csv files.
dataset = pd.read_csv('./flight_delay.csv')

dataset

"""#### Splitting the dataset into features and target."""

x, y = dataset.loc[:, :'Scheduled arrival time'], dataset[['Delay']]

x.isna().sum()

x.head(10)

y.head(10)

types = x.dtypes
print("Number categorical featues:", sum(types=='object'))
print(types)

x.describe()

y.describe()

print(dataset.isna().sum())

"""### Data Preprocessing

Since all the features columns represent objects, we need to find a way to convert them into numeral values. 

- We decided to use one hot encoding with the **departure and destination airports**. Since there is no explicit relation or order between the different airports, i.e. the airports are non-ordinal, one hot encoding is the best strategy to use. 

- For the **scheduled departure times**, we decided to do the following: the *year* part does not have that much of an effect, thus, it is disregard. Moreover, the month and day are represented in a single column called "day order" which ranges from 1 to 365. The time (hours and minutes) is represented again in a single column called (minute order) which ranges from 1 to 24 * 60 = 1440. 

- Finally for hanlding the **scheduled arrival time**, it is enough to represent it as a single column (flight duration) which is simply the difference in minutes between the departure and arrival times.

#### Splitting the data into training and testing set

This step is done now because we are using one hot encoder which is affected if it has access to the testing data.
"""

start_date = "2018-01-01 00:00:00"
xy_train = dataset[(dataset['Scheduled depature time'] < start_date)]
xy_test = dataset[(dataset['Scheduled depature time'] >= start_date)]

x_train, y_train = xy_train.drop(['Delay'], axis=1, inplace=False), xy_train[['Delay']]
x_test, y_test = xy_test.loc[:, :'Scheduled arrival time'], xy_test[['Delay']]
x_train

print(x_train.isna().sum())
print(x_test.isna().sum())
x_train.describe()

"""#### One hot encoding the airports"""

from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures

ohe_feats = ['Depature Airport', 'Destination Airport']
ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)
ohe.fit(x_train[ohe_feats])

def ohe_new_features(df, features_name, ohe):
    new_feats = ohe.transform(df[features_name])
    new_cols = pd.DataFrame(new_feats, dtype=int, index=df.index, columns=ohe.get_feature_names(features_name))
    # display(new_cols)
    new_df = pd.concat([df, new_cols], axis=1)
    new_df.drop(features_name, axis=1, inplace=True)
    return new_df


x_train = ohe_new_features(x_train, ohe_feats, ohe)
x_test =  ohe_new_features(x_test, ohe_feats, ohe)

x.isna().sum().sum()

x_train

x_test

"""#### Handling scheduled arrival time"""

import datetime 

departure_time = list(x_train['Scheduled depature time'].values)
departure_time = [datetime.datetime.fromisoformat(dep) for dep in departure_time]

arrival_time = list(x_train['Scheduled arrival time'].values)
arrival_time = [datetime.datetime.fromisoformat(arr) for arr in arrival_time]

duration_train = [(arr - dep).total_seconds() / 60 for dep, arr in zip(departure_time, arrival_time)]

departure_time = list(x_test['Scheduled depature time'].values)
departure_time = [datetime.datetime.fromisoformat(dep) for dep in departure_time]

arrival_time = list(x_test['Scheduled arrival time'].values)
arrival_time = [datetime.datetime.fromisoformat(arr) for arr in arrival_time]

duration_test = [(arr - dep).total_seconds() / 60 for dep, arr in zip(departure_time, arrival_time)]

x_train['duration'] = duration_train
x_train.drop(['Scheduled arrival time'], axis=1, inplace=True)


x_test['duration'] = duration_test
x_test.drop(['Scheduled arrival time'], axis=1, inplace=True)

x_train

x_test

"""#### Handling scheduled departure time """

import datetime 

departure_time = list(x_train['Scheduled depature time'].values)
departure_time = [datetime.datetime.fromisoformat(dep) for dep in departure_time]
departure_day_of_year = [dep.timetuple().tm_yday for dep in departure_time]
departure_minute = [dep.hour * 60 + dep.minute for dep in departure_time]

x_train['Scheduled depature day'] = departure_day_of_year
x_train['Scheduled depature minute'] = departure_minute
x_train.drop(['Scheduled depature time'], axis=1, inplace=True)

departure_time = list(x_test['Scheduled depature time'].values)
departure_time = [datetime.datetime.fromisoformat(dep) for dep in departure_time]
departure_day_of_year = [dep.timetuple().tm_yday for dep in departure_time]
departure_minute = [dep.hour * 60 + dep.minute for dep in departure_time]

x_test['Scheduled depature day'] = departure_day_of_year
x_test['Scheduled depature minute'] = departure_minute
x_test.drop(['Scheduled depature time'], axis=1, inplace=True)

x_train

x_test

numeral_feats = ['duration', 'Scheduled depature day', 'Scheduled depature minute']

"""### Visualization

#### PCA
"""

x_train_pca = x_train - x_train.mean()
x_test_pca = x_test - x_test.mean()

from sklearn.decomposition import PCA

pca1 = PCA(n_components=1)
x_train_pca = pca1.fit_transform(x_train_pca)

x_test_pca = pca1.transform(x_test_pca)

print("x_train_pca =", x_train_pca)
print("explained variance:", pca1.explained_variance_)
print("explained variance ratio:", pca1.explained_variance_ratio_)

print("x_test_pca =", x_test_pca)

plt.scatter(x_train_pca, y_train, label='train')
plt.scatter(x_test_pca, y_test, label='test')
plt.xlabel("Features")
plt.ylabel("Target")
plt.legend()
plt.show()

"""#### Delay vs Duration"""

plot_x_train = x_train['duration']
plot_x_test = x_test['duration']

plt.scatter(plot_x_train, y_train, label='train')
plt.scatter(plot_x_test, y_test, label='test')
plt.xlabel("duration")
plt.ylabel("delay")
plt.title("delay vs duration")
plt.legend()
plt.show()

"""### Outlier Detection & Removal

Considering the new features from the preproccessing, we can easily see that there cannot be any outliers in the features resultant from the one hot encoding. So we are left with three features. Both day of the year, and minute of the day cannot have outliers either because their ranges are known. Thus, the duration is the only feature that could have extreme values (outliers). Plotting the delay with respect to duration in the previous section has shown as well that there are obviously many extreme points in terms of the duration.

We also need to check the target (duration) if it contains outliers.

#### Detection
"""

import seaborn as sns
sns.boxplot(x=x_train['duration'])
plt.legend(['train duration'])
plt.show()
sns.boxplot(x=x_test['duration'])
plt.legend(['test duration'])
plt.show()
sns.boxplot(x=y_train['Delay'])
plt.legend(['train delay'])
plt.show()
sns.boxplot(x=y_test['Delay'])
plt.legend(['test delay'])
plt.show()
y_test.describe()

from scipy import stats

z_dur = np.abs(stats.zscore(x_train['duration']))
threshold = 3
ext_ind = np.where(z_dur >= threshold)
print("Train duration outliers:", end="")
print(len(ext_ind[0]))

z_del = np.abs(stats.zscore(y_train['Delay']))
ext_ind = np.where(z_del >= threshold)
print("Train delay outliers:", end="")
print(len(ext_ind[0]))

z_dur = np.abs(stats.zscore(x_test['duration']))
threshold = 3
ext_ind = np.where(z_dur >= threshold)
print("Test duration outliers:", end="")
print(len(ext_ind[0]))

z_del = np.abs(stats.zscore(y_test['Delay']))
ext_ind = np.where(z_del >= threshold)
print("Test delay outliers:", end="")
print(len(ext_ind[0]))

"""#### Removal

Since the outliers in this columns do not excced 2% of the training set, we can remove all of them.
"""

xy_train = pd.concat([x_train, y_train], axis=1)

z_dur = np.abs(stats.zscore(xy_train['duration']))
z_del = np.abs(stats.zscore(xy_train['Delay']))

filtered_entries = ((z_dur < threshold) & (z_del < threshold))
print(xy_train.shape)
xy_train = xy_train[filtered_entries]
print(xy_train.shape)

x_train, y_train = xy_train.drop(['Delay'], axis=1, inplace=False), xy_train[['Delay']]
print(x_train.shape, y_train.shape)

x_train.describe()

y_train.describe()

x_test.iloc[:, -3:].describe()

xy_test = pd.concat([x_test, y_test], axis=1)

threshold = 3

z_dur = np.abs(stats.zscore(xy_test['duration']))
z_del = np.abs(stats.zscore(xy_test['Delay']))

filtered_entries = ((z_dur < threshold) & (z_del < threshold))
print(xy_test.shape)
xy_test = xy_test[filtered_entries]
print(xy_test.shape)

x_test, y_test = xy_test.drop(['Delay'], axis=1, inplace=False), xy_test[['Delay']]
print(x_test.shape, y_test.shape)

x_test.iloc[:, -3:].describe()

y_test.describe()

"""#### Visualization after outliers removal

##### PCA
"""

pca1 = PCA(n_components=1)
x_train_pca = x_train - x_train.mean()
x_test_pca = x_test - x_test.mean()
x_train_pca = pca1.fit_transform(x_train_pca)
x_test_pca = pca1.transform(x_test_pca)

plt.scatter(x_train_pca, y_train, label='train', s=2)
plt.scatter(x_test_pca, y_test, label='test', s=2)
plt.xlabel("Features")
plt.ylabel("Target")
plt.legend()
plt.show()

"""##### Delay vs duration"""

plot_x_train = x_train['duration']
plot_x_test = x_test['duration']

plt.scatter(plot_x_train, y_train, label='train', s=2)
plt.scatter(plot_x_test, y_test, label='test', s=2)
plt.xlabel("duration")
plt.ylabel("delay")
plt.title("delay vs duration")
plt.legend()
plt.show()

"""### Machine learning models

#### Linear Regression
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score


linear_regression = LinearRegression()
linear_regression.fit(x_train, y_train)
y_pred = linear_regression.predict(x_test)
y_pred_train = linear_regression.predict(x_train)

# test set
mse_linear = mean_squared_error(y_test, y_pred)
rmse_linear = mean_squared_error(y_test, y_pred, squared=False)

# train set
mse_linear_train = mean_squared_error(y_train, y_pred_train)
rmse_linear_train = mean_squared_error(y_train, y_pred_train, squared=False)

print("Test:")
print("MSE:", mse_linear)
print("RMSE:", rmse_linear)

print("Train:")
print("MSE:", mse_linear_train)
print("RMSE:", rmse_linear_train)

"""#### Linear Regression on PCA"""

pca1 = PCA(n_components=3)
x_train_pca = x_train - x_train.mean()
x_test_pca = x_test - x_test.mean()
x_train_pca = pca1.fit_transform(x_train_pca)
x_test_pca = pca1.transform(x_test_pca)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


linear_regression = LinearRegression()
linear_regression.fit(x_train_pca, y_train)
y_pred = linear_regression.predict(x_test_pca)
y_pred_train = linear_regression.predict(x_train_pca)

# test set
mse_linear_pca = mean_squared_error(y_test, y_pred)
rmse_linear_pca = mean_squared_error(y_test, y_pred, squared=False)

# train set
mse_linear_pca_train = mean_squared_error(y_train, y_pred_train)
rmse_linear_pca_train = mean_squared_error(y_train, y_pred_train, squared=False)

print("Test:")
print("MSE:", mse_linear_pca)
print("RMSE:", rmse_linear_pca)

print("Train:")
print("MSE:", mse_linear_pca_train)
print("RMSE:", rmse_linear_pca_train)

"""#### Linear Regression with Ridge"""

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

ridge = Ridge()
ridge.fit(x_train_pca, y_train)

y_pred = ridge.predict(x_test_pca)

y_pred_train = ridge.predict(x_train_pca)

# test set
mse_ridge = mean_squared_error(y_test, y_pred)
rmse_ridge = mean_squared_error(y_test, y_pred, squared=False)

# train set
mse_ridge_train = mean_squared_error(y_train, y_pred_train)
rmse_ridge_train = mean_squared_error(y_train, y_pred_train, squared=False)

print("Test:")
print("MSE:", mse_ridge)
print("RMSE:", rmse_ridge)

print("Train:")
print("MSE:", mse_ridge_train)
print("RMSE:", rmse_ridge_train)

"""#### Quadratic Regression"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


poly_reg = PolynomialFeatures(degree=2)
X_poly_train = poly_reg.fit_transform(x_train[numeral_feats])

lin_reg = LinearRegression()

X_train = np.hstack((X_poly_train, x_train.drop(numeral_feats, axis=1)))

lin_reg.fit(X_train, y_train)

X_poly_test = poly_reg.transform(x_test[numeral_feats])
X_test = np.hstack((X_poly_test,x_test.drop(numeral_feats ,1)))

y_pred = lin_reg.predict(X_test)

y_pred_train = lin_reg.predict(X_train)

# test set
mse_quad = mean_squared_error(y_test, y_pred)
rmse_quad = mean_squared_error(y_test, y_pred, squared=False)

# train set
mse_quad_train = mean_squared_error(y_train, y_pred_train)
rmse_quad_train = mean_squared_error(y_train, y_pred_train, squared=False)

print("Test:")
print("MSE:", mse_quad)
print("RMSE:", rmse_quad)

print("Train:")
print("MSE:", mse_quad_train)
print("RMSE:", rmse_quad_train)

"""#### Quadratic Regression on PCA"""

pca1 = PCA(n_components=3)
x_train_pca = x_train - x_train.mean()
x_test_pca = x_test - x_test.mean()
x_train_pca = pca1.fit_transform(x_train_pca)
x_test_pca = pca1.transform(x_test_pca)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error

polynomial_features = PolynomialFeatures(degree=2)
linear_regression = LinearRegression()
pipeline = Pipeline([("polynomial_features", polynomial_features),
                      ("linear_regression", linear_regression)])
pipeline.fit(x_train_pca, y_train)

y_pred = pipeline.predict(x_test_pca)
y_pred_train = pipeline.predict(x_train_pca)

# test set
mse_quad_pca = mean_squared_error(y_test, y_pred)
rmse_quad_pca = mean_squared_error(y_test, y_pred, squared=False)

# train set
mse_quad_pca_train = mean_squared_error(y_train, y_pred_train)
rmse_quad_pca_train = mean_squared_error(y_train, y_pred_train, squared=False)

print("Test:")
print("MSE:", mse_quad_pca)
print("RMSE:", rmse_quad_pca)

print("Train:")
print("MSE:", mse_quad_pca_train)
print("RMSE:", rmse_quad_pca_train)

"""#### Quadratic Regression with Ridge"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


poly_reg = PolynomialFeatures(degree=2)
X_poly_train = poly_reg.fit_transform(x_train[numeral_feats])

lin_reg = Ridge()

X_train = np.hstack((X_poly_train, x_train.drop(numeral_feats, axis=1)))

lin_reg.fit(X_train, y_train)

X_poly_test = poly_reg.transform(x_test[numeral_feats])
X_test = np.hstack((X_poly_test,x_test.drop(numeral_feats ,1)))

y_pred = lin_reg.predict(X_test)

y_pred_train = lin_reg.predict(X_train)

# test set
mse_quad_ridge = mean_squared_error(y_test, y_pred)
rmse_quad_ridge = mean_squared_error(y_test, y_pred, squared=False)

# train set
mse_quad_ridge_train = mean_squared_error(y_train, y_pred_train)
rmse_quad_ridge_train = mean_squared_error(y_train, y_pred_train, squared=False)

print("Test:")
print("MSE:", mse_quad_ridge)
print("RMSE:", rmse_quad_ridge)

print("Train:")
print("MSE:", mse_quad_ridge_train)
print("RMSE:", rmse_quad_ridge_train)

"""### Analysis """

plt.figure(figsize=(20,10))
plt.scatter(['Lin Reg', 'Lin Reg PCA', 'Lin Reg Ridge', 'Quad Reg', 'Quad Reg PCA', 'Quad Reg Ridge'], 
            [mse_linear, mse_linear_pca, mse_ridge, mse_quad, mse_quad_pca, mse_quad_ridge])
plt.scatter(['Lin Reg', 'Lin Reg PCA', 'Lin Reg Ridge', 'Quad Reg', 'Quad Reg PCA', 'Quad Reg Ridge'], 
            [rmse_linear, rmse_linear_pca, rmse_ridge, rmse_quad, rmse_quad_pca, rmse_quad_ridge])
plt.scatter(['Lin Reg', 'Lin Reg PCA', 'Lin Reg Ridge', 'Quad Reg', 'Quad Reg PCA', 'Quad Reg Ridge'], 
            [mse_linear_train, mse_linear_pca_train, mse_ridge_train, mse_quad_train, mse_quad_pca_train, mse_quad_ridge_train])
plt.scatter(['Lin Reg', 'Lin Reg PCA', 'Lin Reg Ridge', 'Quad Reg', 'Quad Reg PCA', 'Quad Reg Ridge'], 
            [rmse_linear_train, rmse_linear_pca_train, rmse_ridge_train, rmse_quad_train, rmse_quad_pca_train, rmse_quad_ridge_train])
plt.legend(['MSE', 'RMSE', 'MSE train', 'RMSE train'])

plt.show()

"""From the previous chart and the error values, we can conclude that using quadratic regression produced the best results according to the mean square error metric. However, the difference in the performances was insignificant between the different models. 

Generally speaking all the models achieved satisfactory results having MSE of the range [149, 155]. 
"""